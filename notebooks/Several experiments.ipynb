{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemmy\n",
    "\n",
    "# Create an instance of the standalone lemmatizer.\n",
    "lemmatizer = lemmy.load(\"da\")\n",
    "\n",
    "corpus = \"\"\"\n",
    "                    Frederikshavn Kommune Ønsker du et varieret og spændende job inden for demensafsnittet, \n",
    "                     hvor alle bidrager aktivt i opgaveløsningen? Og har du lyst til at arbejde primært i fast dagvagt \n",
    "                     og have weekendvagt i ulige uger?  Vi søger en medarbejder med sundhedsfaglig baggrund til borgere med demens. \n",
    "                     Stillingen er på gennemsnitlig 30 timer pr. uge. Du kommer både selvstændigt og i samarbejde med \n",
    "                     andre samarbejdspartnere til, at arbejde med udgangspunkt i borgerens behov og værdier. \n",
    "                     Dette sker ud fra den rehabiliterende tilgang hvor du er med til at bevare/forbedre borgernes fysiske, \n",
    "                     psykiske og sociale funktioner. Det er således vigtigt, at det faldet dig naturligt at være fleksibel og\n",
    "                     du er nærværende og tillidsskabende i relationen.\n",
    "                     \"\"\"\n",
    "\n",
    "# Find lemma for the word 'akvariernes'. First argument is an empty POS tag.\n",
    "lemmatizer.lemmatize(\"\", corpus)\n",
    "print(len(corpus.strip()))\n",
    "corpus.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "text = TextBlob(corpus.strip())\n",
    "text.words\n",
    "print(len(text.words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import DanishStemmer\n",
    "\n",
    "stemmer = DanishStemmer()\n",
    "\n",
    "singles = [stemmer.stem(word) for word in text.words]\n",
    "\n",
    "print(len(singles))\n",
    "singles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "da_stop_words = stopwords.words('danish')\n",
    "\n",
    "no_sw = [word for word in singles if not word in da_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(no_sw))\n",
    "\n",
    "no_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(no_sw)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = lemmatizer.lemmatize(\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = TextBlob(lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ipython = get_ipython()\n",
    "\n",
    "def hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n",
    "                   exception_only=False, running_compiled_code=False):\n",
    "    etype, value, tb = sys.exc_info()\n",
    "    return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
    "\n",
    "ipython.showtraceback = hide_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pedromadruga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  7.8s\n",
      "[########################################] | 100% Completed |  8.2s\n",
      "[########################################] | 100% Completed |  8.7s\n",
      "[########################################] | 100% Completed |  9.2s\n",
      "[########################################] | 100% Completed |  9.6s\n",
      "[########################################] | 100% Completed | 13.1s\n",
      "[########################################] | 100% Completed | 13.4s\n",
      "[########################################] | 100% Completed | 13.6s\n",
      "[########################################] | 100% Completed | 13.9s\n",
      "[########################################] | 100% Completed | 14.3s\n",
      "CPU times: user 24.1 s, sys: 1.45 s, total: 25.5 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from pandarallel import pandarallel\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import DanishStemmer\n",
    "import lemmy\n",
    "import multiprocessing\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar(minimum=5).register()\n",
    "\n",
    "\n",
    "# pandarallel.initialize(progress_bar=True, use_memory_fs=None)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = dd.read_csv('../data/interim/jobindex_123k.csv')\n",
    "df = df.repartition(partition_size=\"100MB\")\n",
    "df_init = df\n",
    "\n",
    "df_filtered = df_init[['title', 'company', 'description','location','link','ratings_link', 'source', 'date']]\n",
    "df_filtered['merged'] = (\n",
    "                        df_filtered['title'].fillna('') + ' '\n",
    "                        + df_filtered['company'].fillna('') + ' '\n",
    "                        + df_filtered['description'].fillna('')+ ' '\n",
    "                        + df_filtered['location'].fillna('')+ ' '\n",
    "#                         + df_filtered['link'].astype(str).fillna('')+ ' '\n",
    "                        + df_filtered['ratings_link'].fillna('')+ ' '\n",
    "                        + df_filtered['source'].fillna('')+ ' '\n",
    "                        + df_filtered['date'].astype(str).fillna('')\n",
    "                        )\n",
    "\n",
    "# df['filtered'].compute()[0]\n",
    "\n",
    "da_stop_words = stopwords.words('danish')\n",
    "lemmatizer = lemmy.load(\"da\")\n",
    "stemmer = DanishStemmer()\n",
    "\n",
    "all_descriptions = np.array(df_filtered['merged'])\n",
    "all_descriptions_wo_stopwords = []\n",
    "\n",
    "\n",
    "def pp_text(text):\n",
    "    textblob = TextBlob(text.to_string(index=False))\n",
    "    singles = [stemmer.stem(word) for word in textblob.words]\n",
    "    no_stop_words = [word for word in singles if not word in da_stop_words]\n",
    "    joined_text = \" \".join(no_stop_words)\n",
    "    final_text = lemmatizer.lemmatize(\"\", joined_text)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "# df_filtered['processed'] = df_filtered['merged'].map_partitions(pp_text, meta=df_filtered['merged']).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[######################                  ] | 55% Completed |  6min 36.2s\n",
      "[######################                  ] | 55% Completed |  6min 36.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[######################                  ] | 55% Completed |  6min 38.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[######################                  ] | 55% Completed |  6min 38.4s"
     ]
    }
   ],
   "source": [
    "df_filtered['processed'] = df_filtered['merged'].map_partitions(pp_text, meta=df_filtered['merged']).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_filtered))\n",
    "\n",
    "print(df_filtered['processed'].compute()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", dtype=\"float32\")\n",
    "X = vectorizer.fit_transform(df_filtered['merged'].compute())\n",
    "\n",
    "# print(df_filtered['merged'].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_kernels\n",
    "def search(job_title):\n",
    "        \n",
    "    stuff = [df_filtered.compute().index[df_filtered['merged'].str.contains(word, case=False).compute()].values for word in job_title.split()]\n",
    "    return list(set(stuff[0]).intersection(*stuff))\n",
    "\n",
    "search_results = search(\"jurist forsikring københavn\")\n",
    "print(f\"number of direct results: {len(search_results)}\")\n",
    "\n",
    "# print(df['title'][first_result])\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# for result in search_results:\n",
    "#     print(df['title'][result])\n",
    "\n",
    "\n",
    "%time distances = pairwise_kernels(X, X[search_results[1]], n_jobs=-1)\n",
    "\n",
    "for index, search_result in enumerate(search_results):\n",
    "    distances = pairwise_kernels(X, X[search_results[index]], n_jobs=-1)\n",
    "    for _index, value in enumerate(distances):\n",
    "        recommendations.append({\n",
    "            \"title\": df['title'][_index],\n",
    "            \"similarity\": value[0],\n",
    "            \"location\": df['location'][_index],\n",
    "            \"description\": df['description'][_index]\n",
    "        })\n",
    "\n",
    "sorted_similarities = sorted(recommendations, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "for index, result in enumerate(sorted_similarities):\n",
    "    print(result)\n",
    "    print('\\n')\n",
    "    if index > 20:\n",
    "        break\n",
    "    \n",
    " \n",
    "# sorted_similarities = sorted(range(len(distances)), key=lambda k: distances[k], reverse=True)[:100]\n",
    "\n",
    "# for index,value in tqdm(enumerate(sorted_similarities)):\n",
    "#     print('\\n')\n",
    "#     print(df['title'][value])\n",
    "#     print(df['description'][value])\n",
    "#     print(df['location'][value])\n",
    "#     print(distances[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word2vec\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "def tokenize(text):\n",
    "    stringified_text = text\n",
    "#     print(type(stringified_text))\n",
    "    tokenized_text = TextBlob(stringified_text)\n",
    "    return tokenized_text.words\n",
    "#     return text\n",
    "\n",
    "# df_filtered['merged_tokenized'] = df_filtered['merged']\n",
    "\n",
    "#     test_f, \n",
    "#     args=('col_1', 'col_2'), \n",
    "#     axis=1, \n",
    "#     meta=('result', int)\n",
    "\n",
    "df_filtered[\"tokenized_merged\"] = df_filtered.map_partitions(\n",
    "    lambda df_filtered:\\\n",
    "    df_filtered.apply(\\\n",
    "    (lambda row: tokenize(row['merged'])), axis=1))\\\n",
    "    .compute()\n",
    "\n",
    "df_filtered['tokenized_merged']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_filtered['stuff'].compute(scheduler='processes')\n",
    "%time df_filtered['stuff'].compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# similarity = cosine_similarity(X)\n",
    "\n",
    "def compute_cosine_distances(a, b):\n",
    "\n",
    "    normalize_a = tf.nn.l2_normalize(a,1)        \n",
    "    normalize_b = tf.nn.l2_normalize(b,1)\n",
    "    distance = 1 - tf.matmul(normalize_a, normalize_b, transpose_b=True)\n",
    "\n",
    "    return distance\n",
    "\n",
    "# def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "#     coo = X.tocoo()\n",
    "#     indices = np.mat([coo.row, coo.col]).transpose()\n",
    "#     return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "# ####################### ####################### ####################### \n",
    "\n",
    "# x_train = convert_sparse_matrix_to_sparse_tensor(X)\n",
    "\n",
    "input_matrix = X.toarray()\n",
    "\n",
    "# %time similarity_by_tf = compute_cosine_distances(input_matrix, input_matrix)\n",
    "%time similarity = cosine_similarity(X)\n",
    "\n",
    "# similarity_by_tf\n",
    "\n",
    "# %time print(cosine_similarity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_distance import cosine\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "distance_matrix = np.zeros(X.shape)\n",
    "\n",
    "# print(X.T)\n",
    "\n",
    "for i in X:\n",
    "    for j in X.T:\n",
    "        print(cosine(i,j))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df = pd.DataFrame(np.random.rand(1000,100))\n",
    "df['distances'] = cosine_similarity(df, df.iloc[0:1]) # Here I assume that the parent vector is stored as the first row in the dataframe, but you could also store it separately\n",
    "\n",
    "n = 10 # or however many you want\n",
    "n_largest = df['distances'].nlargest(n + 1)\n",
    "\n",
    "n_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [2, 3, 1, 4, 5]\n",
    "sorted(range(len(s)), key=lambda k: s[k], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tzlocal()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 3, 1, 0, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "HERE = tz.tzlocal()\n",
    "\n",
    "print(HERE)\n",
    "\n",
    "datetime.datetime(2020,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=1903, tm_mon=2, tm_mday=1, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=6, tm_yday=32, tm_isdst=-1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "import time\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "time.strptime('Dom, 01/02/1903', '%a, %d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-da05409c9e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{os.path.abspath(os.getcwd())}/data/processed/{dataset_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_name' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f'{os.path.abspath(os.getcwd())}/data/processed/{dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8805c2b3fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
